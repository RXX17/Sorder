{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14rMX_GzovAUKq8ZcOCVXjqxzP4o51Csk",
      "authorship_tag": "ABX9TyOTKBAzJ0eN6Y8MO1scuS8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RXX17/Sorder/blob/main/WhatsappSorder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation (Must after opening script)"
      ],
      "metadata": {
        "id": "_lkmzXuQDpya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo1ja2uIDGq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feec55d9-0781-4b03-bd43-27b55dc8254e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newspaper3k"
      ],
      "metadata": {
        "id": "o_-Hd-nRDXMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d3d54a-7b99-4cdc-dc2a-d1aa2b78ae8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.2/211.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=ffd458bbd6dde61b93a618aa81be46012aaba18a1107565105dd126bb5959441\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3339 sha256=b7712120257134a21024bbf6abdcd961552cd5f3b9445cf7fab1873b8a8184de\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=aae8a7663586553b72d02e02b23b24f30a085027595e21cd0d50bd69ea03c545\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=2968e5e494a3bd1b029e16a807b78615e6fea8e831adbdab1576b03e794d6d5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytube"
      ],
      "metadata": {
        "id": "NfJF6Uy-DePr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cc6be3-f069-48cb-9b8a-674bdacd4810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install reportlab"
      ],
      "metadata": {
        "id": "Bsq1zH_IDhtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fa2acd-96d5-4a4d-8cfb-e4e078dc915c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.0.7-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.4.0)\n",
            "Installing collected packages: reportlab\n",
            "Successfully installed reportlab-4.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "Aye_dX4BDtYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper # get article\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer # summary\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import requests\n",
        "\n",
        "# for YouTube\n",
        "from pytube import YouTube\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# for PDF\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet"
      ],
      "metadata": {
        "id": "vPGGjV3RDr5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapper"
      ],
      "metadata": {
        "id": "i1ik_jWGD0qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_am_pm_url_lines_from_file(input_file, output_file):\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    output_lines = []\n",
        "    for line in lines:\n",
        "        date_time_match = re.search(r'\\d{1,2}/\\d{1,2}/\\d{2}, \\d{1,2}:\\d{2}\\s*[AP]M', line)\n",
        "        if date_time_match:\n",
        "            date_time = date_time_match.group()\n",
        "            url = re.search(r'(https?://\\S+)', line)\n",
        "            if url:\n",
        "                output_lines.append(f\"{date_time} - {url.group()}\")\n",
        "\n",
        "    with open(output_file, 'w') as file:\n",
        "        for line in output_lines:\n",
        "            file.write(line + '\\n')\n",
        "\n",
        "    print(f\"Output saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "input_file = '/content/drive/MyDrive/WhatsAppChat/Raisa WhatsApp Chat with Rafik Research.txt'\n",
        "output_file = '/content/drive/MyDrive/WhatsAppChat/RaisaWhatsAppChatwithRafikResearch.txt'\n",
        "\n",
        "save_am_pm_url_lines_from_file(input_file, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDpt9CMyMBmA",
        "outputId": "01e13047-8c7a-4bee-8f9f-c6a03c6db9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to /content/drive/MyDrive/WhatsAppChat/RaisaWhatsAppChatwithRafikResearch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizer"
      ],
      "metadata": {
        "id": "Ib-SJYAJENk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Article"
      ],
      "metadata": {
        "id": "Tnk1r7MdERW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_text(url):\n",
        "    if \"youtube.com\" in url or \"youtu.be\" in url:\n",
        "        return get_youtube_description(url)\n",
        "\n",
        "    if re.search(r\"(maps\\.google\\.com)|(maps\\.apple\\.com)|(openstreetmap\\.org)\", url):\n",
        "        return \"URL contains location data\"\n",
        "\n",
        "    if \"docs.google.com\" in url:\n",
        "        # Fetch the content of the Google Docs document\n",
        "        doc_content = get_google_docs_content(url)\n",
        "\n",
        "        # Return the document content\n",
        "        return doc_content\n",
        "\n",
        "    # Initialize newspaper with the URL\n",
        "    article = newspaper.Article(url)\n",
        "\n",
        "    try:\n",
        "        # Download and parse the article\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        # Check if the article text is empty\n",
        "        if not article.text:\n",
        "            return \"Model cannot access the content\"\n",
        "\n",
        "        # Return the article text\n",
        "        return article.text[:1000]\n",
        "\n",
        "    except newspaper.article.ArticleException:\n",
        "        # Return \"no details provided\" for URLs without articles\n",
        "        return \"no details provided\""
      ],
      "metadata": {
        "id": "_OY2qoFtEPWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YouTube Description"
      ],
      "metadata": {
        "id": "T2ELy1_kEZ-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_youtube_description(url):\n",
        "    try:\n",
        "        # Extract the video ID from the URL\n",
        "        video_id = extract_video_id(url)\n",
        "\n",
        "        # If video_id is None, it indicates a link to a channel\n",
        "        if video_id is None:\n",
        "            return \"Link to a channel\"\n",
        "\n",
        "        # Create a YouTube Data API client\n",
        "        # API Key\n",
        "        api_service_name = \"youtube\"\n",
        "        api_version = \"v3\"\n",
        "        api_key = \"API Key\"  # Replace with your YouTube Data API key\n",
        "        youtube = build(api_service_name, api_version, developerKey=api_key)\n",
        "\n",
        "        # Retrieve video details using the video ID\n",
        "        video_request = youtube.videos().list(\n",
        "            part=\"snippet\",\n",
        "            id=video_id\n",
        "        )\n",
        "        video_response = video_request.execute()\n",
        "\n",
        "        # Get the video description from the response\n",
        "        if 'items' in video_response and video_response['items']:\n",
        "            description = video_response['items'][0]['snippet']['description']\n",
        "            return description\n",
        "\n",
        "    except HttpError as e:\n",
        "        # Return an error message if there is any issue\n",
        "        return \"Error: \" + str(e)"
      ],
      "metadata": {
        "id": "OTVHbphgEbz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_id(url):\n",
        "    # Extract the video ID from various YouTube URL formats\n",
        "    video_id = None\n",
        "    if \"youtube.com\" in url or \"youtu.be\" in url:\n",
        "        if \"youtube.com/watch\" in url:\n",
        "            video_id = url.split(\"?v=\")[1].split(\"&\")[0]\n",
        "        elif \"youtu.be/\" in url:\n",
        "            video_id = url.split(\"/\")[-1]\n",
        "    return video_id"
      ],
      "metadata": {
        "id": "X69DHYWmEhuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docs Description"
      ],
      "metadata": {
        "id": "FoU6XGf2Ei8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_google_docs_content(url):\n",
        "    # Extract the document ID from the URL\n",
        "    doc_id = url.split('/')[-2]\n",
        "\n",
        "    # Make a request to the Google Docs API\n",
        "    api_key = 'AIzaSyAiS-8oe0PtPADZhwOGpDujbTtcxLSSQZw'  # Replace with your actual API key\n",
        "    endpoint = f\"https://docs.googleapis.com/v1/documents/{doc_id}?key={api_key}\"\n",
        "    response = requests.get(endpoint)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract the content from the API response\n",
        "        content = response.json()['body']['content']\n",
        "\n",
        "        # Process and return the content\n",
        "        doc_content = \"\"\n",
        "        for element in content:\n",
        "            if 'paragraph' in element:\n",
        "                paragraph = element['paragraph']\n",
        "                text = ''.join([run['text'] for run in paragraph['elements']])\n",
        "                doc_content += text\n",
        "\n",
        "        return doc_content\n",
        "\n",
        "    else:\n",
        "        return \"Unable to fetch Google Docs content\""
      ],
      "metadata": {
        "id": "F4TvUMhpEkYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization"
      ],
      "metadata": {
        "id": "_dk2P4sDEoqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_article(article_text):\n",
        "    # Load BART model and tokenizer\n",
        "    model_name = 'facebook/bart-large-cnn'\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize and encode the input text\n",
        "    inputs = tokenizer.encode_plus(article_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
        "\n",
        "    # Decode the summary tokens back into text\n",
        "    summary_text = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "    return summary_text"
      ],
      "metadata": {
        "id": "o9b8qOFqEoG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example case"
      ],
      "metadata": {
        "id": "_vY2dXbbVJKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "url = \"https://youtu.be/kZbOOzMREWA?list=TLGGVZwh8qO9VIcwNjA3MjAyMw\"  # Replace with your desired URL\n",
        "\n",
        "# Get the article text\n",
        "article_text = get_article_text(url)\n",
        "\n",
        "# Summarize the article text\n",
        "summary = summarize_article(article_text)\n",
        "\n",
        "# Print the URL and its summary\n",
        "print(f\"URL: {url}\")\n",
        "print(f\"Summary: {summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "peEaPNngVK8G",
        "outputId": "fe02a031-a1dd-4edd-a351-a418f826d72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-42dc5a33b2c6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Summarize the article text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print the URL and its summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-30b43352e2cd>\u001b[0m in \u001b[0;36msummarize_article\u001b[0;34m(article_text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Tokenize and encode the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Generate the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2754\u001b[0m         )\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2757\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    633\u001b[0m                     )\n\u001b[1;32m    634\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    636\u001b[0m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                         \u001b[0;34m\" integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input None is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final PDF with summaries"
      ],
      "metadata": {
        "id": "yKqM-ChfIjrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "JXqlifHvQexI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
        "\n",
        "from reportlab.lib.enums import TA_LEFT\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.utils import ImageReader\n",
        "from io import BytesIO\n",
        "from urllib.parse import urlparse\n",
        "import webbrowser"
      ],
      "metadata": {
        "id": "SVTVf50fCeTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization and Saving to PDF"
      ],
      "metadata": {
        "id": "r5IPy6frQlCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize the whole document\n",
        "def summarize_whole_document(lines, output_file_path):\n",
        "    # Create a list to hold the document content\n",
        "    content = []\n",
        "\n",
        "    # Define the document style\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Define the missing styles (or customize as needed)\n",
        "    date_style = ParagraphStyle(\n",
        "        \"DateStyle\",\n",
        "        parent=styles[\"Normal\"],\n",
        "        textColor=colors.black,\n",
        "        fontSize=12,\n",
        "        spaceAfter=5,\n",
        "    )\n",
        "\n",
        "    time_style = ParagraphStyle(\n",
        "        \"TimeStyle\",\n",
        "        parent=styles[\"Normal\"],\n",
        "        textColor=colors.black,\n",
        "        fontSize=12,\n",
        "        spaceAfter=5,\n",
        "    )\n",
        "\n",
        "    url_style = ParagraphStyle(\n",
        "        \"URLStyle\",\n",
        "        parent=styles[\"Normal\"],\n",
        "        textColor=colors.blue,\n",
        "        fontSize=12,\n",
        "        spaceAfter=5,\n",
        "        underline=True,\n",
        "    )\n",
        "\n",
        "    summary_style = ParagraphStyle(\n",
        "        \"SummaryStyle\",\n",
        "        parent=styles[\"Normal\"],\n",
        "        textColor=colors.black,\n",
        "        fontSize=12,\n",
        "        spaceAfter=10,\n",
        "    )\n",
        "\n",
        "    link_style = ParagraphStyle(\n",
        "        \"LinkStyle\",\n",
        "        parent=styles[\"Normal\"],\n",
        "        textColor=colors.blue,\n",
        "        underline=True,\n",
        "        spaceAfter=10,\n",
        "        alignment=TA_LEFT,\n",
        "        backColor=colors.white,\n",
        "        borderWidth=0,\n",
        "    )\n",
        "\n",
        "    # Initialize progress bar\n",
        "    progress_bar = tqdm(total=len(lines), desc=\"Processing Links\")\n",
        "\n",
        "    # Extract and process URLs, dates, and times\n",
        "    for line in lines:\n",
        "        # Start timer for each link processing\n",
        "        link_start_time = time.time()\n",
        "\n",
        "        date_match = re.search(r'\\d{1,2}/\\d{1,2}/\\d{2}', line)\n",
        "        time_match = re.search(r'(\\d{1,2}:\\d{2}\\s*[AP]M|\\d{1,2}:\\d{2})', line)\n",
        "        url_match = re.search(r\"https?://[^\\s/$.?#].[^\\s]*\", line)\n",
        "\n",
        "        url = None  # Initialize 'url' variable\n",
        "\n",
        "        if date_match:\n",
        "            date = date_match.group()\n",
        "            content.append(Paragraph(f\"<b>Date:</b> {date}\", date_style))\n",
        "            parsed_url = urlparse(url)\n",
        "\n",
        "        if time_match:\n",
        "            link_time = time_match.group()\n",
        "            content.append(Paragraph(f\"<b>Time:</b> {link_time}\", time_style))\n",
        "\n",
        "        if url_match:\n",
        "            url = url_match.group()\n",
        "            parsed_url = urlparse(url)\n",
        "            # content.append(Paragraph(f\"<b>URL:</b> {url}\", url_style))\n",
        "\n",
        "            # Make the URL clickable\n",
        "            url_text = f'<u><font color=\"blue\"><a href=\"{url}\">{parsed_url.netloc + parsed_url.path}</a></font></u>'\n",
        "            content.append(Paragraph(url_text, link_style))\n",
        "\n",
        "            # Get the article text\n",
        "            article_text = get_article_text(url)\n",
        "\n",
        "            if article_text == \"no details provided\":\n",
        "                summary = \"No details provided\"\n",
        "            elif article_text == \"Youtube link\":\n",
        "                summary = \"Youtube link\"\n",
        "            elif article_text == \"Link to a channel\":\n",
        "                summary = \"Link to a channel\"\n",
        "            elif article_text == \"Model cannot access the content\":\n",
        "                summary = \"Model cannot access the content\"\n",
        "            elif article_text == \"URL contains location data\":\n",
        "                summary = \"URL contains location data\"\n",
        "            elif article_text == \"Unable to fetch Google Docs content\":\n",
        "                summary = \"The URL contains Google Docs content. Model is unable to fetch Google Docs content\"\n",
        "            else:\n",
        "                # Summarize the article text\n",
        "                summary = summarize_article(article_text)\n",
        "\n",
        "            # content.append(Paragraph(f\"<b>URL:</b> {url}\", url_style))\n",
        "            content.append(Paragraph(f\"<b>Summary:</b>\", summary_style))\n",
        "            content.append(Paragraph(summary, styles[\"Normal\"]))\n",
        "            content.append(Paragraph(\"<br/>\", styles[\"Normal\"]))\n",
        "\n",
        "        # Calculate time taken for each link processing\n",
        "        link_end_time = time.time()\n",
        "        link_processing_time = link_end_time - link_start_time\n",
        "\n",
        "        # Update progress bar description\n",
        "        progress_bar.set_description(f\"Processing Links | Time: {link_processing_time:.2f}s\")\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.update()\n",
        "\n",
        "    # Close progress bar\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Create the PDF document\n",
        "    doc = SimpleDocTemplate(output_file_path, pagesize=letter)\n",
        "\n",
        "    # Build the document content\n",
        "    doc.build(content)"
      ],
      "metadata": {
        "id": "bnE91S4KOQDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Date Range"
      ],
      "metadata": {
        "id": "edvCLab0Qp5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize URLs within a fixed date range\n",
        "def summarize_urls_within_date_range(lines, output_file_path, from_date, to_date):\n",
        "    # Filter lines to get URLs within the specified date range\n",
        "    filtered_lines = [line for line in lines if re.search(r'\\d{1,2}/\\d{1,2}/\\d{2}', line)]\n",
        "    filtered_lines = [line for line in filtered_lines if from_date <= re.search(r'\\d{1,2}/\\d{1,2}/\\d{2}', line).group() <= to_date]\n",
        "\n",
        "    # Summarize URLs within the date range\n",
        "    summarize_whole_document(filtered_lines, output_file_path)\n",
        "\n",
        "# Read URLs from a text file\n",
        "file_path = '/content/drive/MyDrive/WhatsAppChat/RaisaWhatsAppChatwithRafikResearch.txt'\n",
        "output_file_path = '/content/drive/MyDrive/WhatsAppChat/RaisaWhatsAppChatwithRafikResearch.pdf'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Ask the user for their preference\n",
        "choice = input(\"Do you want to summarize the whole document (Y/N)? \").lower()\n",
        "\n",
        "if choice == 'y':\n",
        "    # Summarize the whole document\n",
        "    summarize_whole_document(lines, output_file_path)\n",
        "elif choice == 'n':\n",
        "    # Summarize URLs within a fixed date range\n",
        "    from_date = input(\"Enter the start date (MM/DD/YY): \")\n",
        "    to_date = input(\"Enter the end date (MM/DD/YY): \")\n",
        "    summarize_urls_within_date_range(lines, output_file_path, from_date, to_date)\n",
        "else:\n",
        "    print(\"Invalid choice. Please enter 'Y' or 'N'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_to9PEgMqOY",
        "outputId": "5cb84e2a-60e8-45db-88ad-f0f6d4432282"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to summarize the whole document (Y/N)? y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Links | Time: 1.12s: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [04:32<00:00, 19.44s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works perfectly till now ðŸ™‚"
      ],
      "metadata": {
        "id": "Perg8IetFPZF"
      }
    }
  ]
}